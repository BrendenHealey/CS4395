{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Wordnet is an NLTK corpus reader that can look up words using synsets"
      ],
      "metadata": {
        "id": "qmafstYwrH_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_bpH9_AlqyzV",
        "outputId": "7b9d2dc8-4a01-48da-a136-751c65c799f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "from nltk.book import *\n",
        "\n",
        "nltk.download('book')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Output all synsets of 'cat' as well as the definition, examples, and lemmas of one synset. Then traverse the hierarchy as high as you can"
      ],
      "metadata": {
        "id": "9G3gXZlCrIrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synsets = wn.synsets('cat')\n",
        "print(\"All synsets:\", synsets)\n",
        "print()\n",
        "synset = synsets[0]\n",
        "\n",
        "print(\"Synset:\", synset)\n",
        "print(\"Definition:\", synset.definition())\n",
        "print(\"Examples:\", synset.examples())\n",
        "print(\"Lemmas:\", synset.lemmas())\n",
        "\n",
        "top = wn.synset('entity.n.01')\n",
        "while synset:\n",
        "    print(synset)\n",
        "    if synset == top:\n",
        "        break\n",
        "    if synset.hypernyms():\n",
        "        synset = synset.hypernyms()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "krLp5Mv9q6JA",
        "outputId": "34577583-965a-467c-f590-d1a3e9702f22"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All synsets: [Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n",
            "\n",
            "Synset: Synset('cat.n.01')\n",
            "Definition: feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\n",
            "Examples: []\n",
            "Lemmas: [Lemma('cat.n.01.cat'), Lemma('cat.n.01.true_cat')]\n",
            "Synset('cat.n.01')\n",
            "Synset('feline.n.01')\n",
            "Synset('carnivore.n.01')\n",
            "Synset('placental.n.01')\n",
            "Synset('mammal.n.01')\n",
            "Synset('vertebrate.n.01')\n",
            "Synset('chordate.n.01')\n",
            "Synset('animal.n.01')\n",
            "Synset('organism.n.01')\n",
            "Synset('living_thing.n.01')\n",
            "Synset('whole.n.02')\n",
            "Synset('object.n.01')\n",
            "Synset('physical_entity.n.01')\n",
            "Synset('entity.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Output the hypernyms, hyponyms, meronyms, holonyms, and antonyms of that synset"
      ],
      "metadata": {
        "id": "bkRA6Nnru1lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hypernyms:\", synset.hypernyms())\n",
        "print(\"Hyponyms:\", synset.hyponyms())\n",
        "print(\"Meronyms:\", synset.part_meronyms())\n",
        "print(\"Holonyms:\", synset.part_holonyms())\n",
        "print(\"Antonyms:\", synset.lemmas()[0].antonyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9Yfjct-rq6eu",
        "outputId": "f467d63b-1c16-4d3d-ce74-4ffb3cc9cda3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms: []\n",
            "Hyponyms: [Synset('abstraction.n.06'), Synset('physical_entity.n.01'), Synset('thing.n.08')]\n",
            "Meronyms: []\n",
            "Holonyms: []\n",
            "Antonyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select a verb and output synsets, the definition, examples, lemmas, and then traverse the hierarchy as high as you can"
      ],
      "metadata": {
        "id": "lWvPqG3fvvwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = wn.synsets('climb')\n",
        "print(\"All synsets:\", verbs)\n",
        "print()\n",
        "synset = verbs[0]\n",
        "\n",
        "print(\"Synset:\", synset)\n",
        "print(\"Definition:\", synset.definition())\n",
        "print(\"Examples:\", synset.examples())\n",
        "print(\"Lemmas:\", synset.lemmas())\n",
        "\n",
        "top = wn.synset('entity.n.01')\n",
        "while synset:\n",
        "    print(synset)\n",
        "    if synset == top:\n",
        "        break\n",
        "    if synset.hypernyms():\n",
        "        synset = synset.hypernyms()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VUijMV2EvwLl",
        "outputId": "fe83062c-24e2-4675-c4e8-6087fcd5b152"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All synsets: [Synset('ascent.n.01'), Synset('climb.n.02'), Synset('climb.n.03'), Synset('climb.v.01'), Synset('climb.v.02'), Synset('wax.v.02'), Synset('climb.v.04'), Synset('climb.v.05'), Synset('rise.v.02')]\n",
            "\n",
            "Synset: Synset('ascent.n.01')\n",
            "Definition: an upward slope or grade (as in a road)\n",
            "Examples: [\"the car couldn't make it up the rise\"]\n",
            "Lemmas: [Lemma('ascent.n.01.ascent'), Lemma('ascent.n.01.acclivity'), Lemma('ascent.n.01.rise'), Lemma('ascent.n.01.raise'), Lemma('ascent.n.01.climb'), Lemma('ascent.n.01.upgrade')]\n",
            "Synset('ascent.n.01')\n",
            "Synset('slope.n.01')\n",
            "Synset('geological_formation.n.01')\n",
            "Synset('object.n.01')\n",
            "Synset('physical_entity.n.01')\n",
            "Synset('entity.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use morphy to find different forms of the word"
      ],
      "metadata": {
        "id": "65vd1fjJvwZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.morphy('climb'))\n",
        "print(wn.morphy('climbing'))\n",
        "print(wn.morphy('climbs'))\n",
        "print(wn.morphy('climbed'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qoXuqnuPvwmg",
        "outputId": "2b919e39-d819-4974-ad03-12e07e21aed7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "climb\n",
            "climbing\n",
            "climb\n",
            "climb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select two words that you think might be similar. Find the specific synsets you are interested in. Run the Wu-Palmer similarity metric and the Lesk algorithm. "
      ],
      "metadata": {
        "id": "_vwDCgFRz3AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigsyn = wn.synsets('big')\n",
        "big = bigsyn[1]\n",
        "largesyn = wn.synsets('large')\n",
        "large = largesyn[2]\n",
        "\n",
        "print(big)\n",
        "print(big.definition())\n",
        "print()\n",
        "print(large)\n",
        "print(large.definition())\n",
        "print()\n",
        "\n",
        "# Wu-Palmer similarity\n",
        "wn.wup_similarity(big, large)\n",
        "\n",
        "# Lesk algo\n",
        "sent = word_tokenize(\"The empire had grown large\")\n",
        "print(lesk(sent, 'big'))\n",
        "print(wn.synset('large.a.01').definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5doi9s0ez5ys",
        "outputId": "e98b29ea-33f1-4afa-c757-fca4ed95c8d3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('big.s.02')\n",
            "significant\n",
            "\n",
            "Synset('large.s.02')\n",
            "fairly large or important in effect; influential\n",
            "\n",
            "Synset('large.a.01')\n",
            "above average in size or number or quantity or magnitude or extent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select an emotionally charged word. Find its senti-synsets and output the polarity scores for each word. Make up a sentence. Output the polarity for each word in the sentence."
      ],
      "metadata": {
        "id": "i449b8H16a1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiwordnet analysis\n",
        "emote = 'rage'\n",
        "sentisynsets = list(swn.senti_synsets(emote))\n",
        "print('Sentisynsets:')\n",
        "for synset in sentisynsets:\n",
        "  print(synset)\n",
        "  print(\"Pos:\", synset.pos_score(), 'Neg:', synset.neg_score(), 'Obj:', synset.obj_score())\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sZtalF9E6hGA",
        "outputId": "d31131e0-4797-4d1b-d002-5cafe819c4e5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentisynsets:\n",
            "<fury.n.01: PosScore=0.25 NegScore=0.5>\n",
            "Pos: 0.25 Neg: 0.5 Obj: 0.25\n",
            "\n",
            "<rage.n.02: PosScore=0.0 NegScore=0.125>\n",
            "Pos: 0.0 Neg: 0.125 Obj: 0.875\n",
            "\n",
            "<rage.n.03: PosScore=0.625 NegScore=0.0>\n",
            "Pos: 0.625 Neg: 0.0 Obj: 0.375\n",
            "\n",
            "<rage.n.04: PosScore=0.0 NegScore=0.125>\n",
            "Pos: 0.0 Neg: 0.125 Obj: 0.875\n",
            "\n",
            "<fad.n.01: PosScore=0.25 NegScore=0.0>\n",
            "Pos: 0.25 Neg: 0.0 Obj: 0.75\n",
            "\n",
            "<ramp.v.01: PosScore=0.0 NegScore=0.0>\n",
            "Pos: 0.0 Neg: 0.0 Obj: 1.0\n",
            "\n",
            "<rage.v.02: PosScore=0.0 NegScore=0.5>\n",
            "Pos: 0.0 Neg: 0.5 Obj: 0.5\n",
            "\n",
            "<rage.v.03: PosScore=0.0 NegScore=0.5>\n",
            "Pos: 0.0 Neg: 0.5 Obj: 0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word polarity\n",
        "sent = 'The quick brown fox jumped over the lazy dog.'\n",
        "tokens = word_tokenize(sent)\n",
        "for token in tokens:\n",
        "  token_synsets = list(swn.senti_synsets(token))\n",
        "  if token_synsets:\n",
        "    print(token_synsets[0], 'Pos:', token_synsets[0].pos_score(), 'Neg:', token_synsets[0].neg_score(), 'Obj:', token_synsets[0].obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "70-GJMyX7ygf",
        "outputId": "6095b955-2a6e-4938-8c45-9b30336e4198"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<quick.n.01: PosScore=0.0 NegScore=0.0> Pos: 0.0 Neg: 0.0 Obj: 1.0\n",
            "<brown.n.01: PosScore=0.0 NegScore=0.375> Pos: 0.0 Neg: 0.375 Obj: 0.625\n",
            "<fox.n.01: PosScore=0.0 NegScore=0.0> Pos: 0.0 Neg: 0.0 Obj: 1.0\n",
            "<jump.v.01: PosScore=0.0 NegScore=0.0> Pos: 0.0 Neg: 0.0 Obj: 1.0\n",
            "<over.n.01: PosScore=0.0 NegScore=0.0> Pos: 0.0 Neg: 0.0 Obj: 1.0\n",
            "<lazy.s.01: PosScore=0.0 NegScore=0.0> Pos: 0.0 Neg: 0.0 Obj: 1.0\n",
            "<dog.n.01: PosScore=0.0 NegScore=0.0> Pos: 0.0 Neg: 0.0 Obj: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Output collocations for text4, the Inaugural corpus. Select one of the collocations identified by NLTK. Calculate mutual information"
      ],
      "metadata": {
        "id": "kOm1Qo9h8k0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colloc = text4.collocations()\n",
        "print(colloc)\n",
        "\n",
        "#prob of vice pres\n",
        "print()\n",
        "length = len(set(text4))\n",
        "vicePres = text.count('Vice President') / length\n",
        "print('Probability of vice president:', vicePres)\n",
        "justVice = text.count('Vice') / length\n",
        "print('Probability of vice:', justVice)\n",
        "justPres = text.count('President') / length\n",
        "print('Probability of president:', justPres)\n",
        "\n",
        "print()\n",
        "pmi = math.log2(vicePres / (justVice * justPres))\n",
        "print('PMI:', pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fcsjpEGz8sBZ",
        "outputId": "ca2d801f-7d11-45fd-e0f5-caffc41714f3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "None\n",
            "\n",
            "Probability of vice president: 0.0017955112219451373\n",
            "Probability of vice: 0.0018952618453865336\n",
            "Probability of president: 0.010773067331670824\n",
            "\n",
            "PMI: 6.458424602064904\n"
          ]
        }
      ]
    }
  ]
}